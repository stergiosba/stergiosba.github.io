"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[619],{144:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>s,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"Swarm_pub/tase","title":"On Swarm Leader Identification using Probing Policies","description":"Journal: IEEE Transactions on Automation Science and Engineering (T-ASE)","source":"@site/docs/Swarm_pub/tase.md","sourceDirName":"Swarm_pub","slug":"/Swarm_pub/tase","permalink":"/docs/Swarm_pub/tase","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Swarm_pub/tase.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Learning Adversarial Policies for Swarm Leader Identification Using a Probing Agent","permalink":"/docs/Swarm_pub/icra"}}');var r=n(4848),t=n(8453);const s={sidebar_position:5},o="On Swarm Leader Identification using Probing Policies",c={},d=[{value:"Abstract:",id:"abstract",level:2}];function l(e){const i={h1:"h1",h2:"h2",header:"header",p:"p",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.header,{children:(0,r.jsx)(i.h1,{id:"on-swarm-leader-identification-using-probing-policies",children:"On Swarm Leader Identification using Probing Policies"})}),"\n",(0,r.jsx)(i.p,{children:"Journal: IEEE Transactions on Automation Science and Engineering (T-ASE)"}),"\n",(0,r.jsx)(i.p,{children:"Date of Conference: 19-23 May 2025"}),"\n",(0,r.jsx)(i.h2,{id:"abstract",children:"Abstract:"}),"\n",(0,r.jsx)(i.p,{children:"Identifying the leader within a robotic swarm is crucial, especially in adversarial contexts where leader concealment is necessary for mission success. This work introduces the interactive Swarm Leader Identification (iSLI) problem, a novel approach where an adversarial probing agent identifies a swarm's leader by physically interacting with its members. We formulate the iSLI problem as a Partially Observable Markov Decision Process (POMDP)  and employ Deep Reinforcement Learning, specifically Proximal Policy Optimization (PPO), to train the prober's policy. The proposed approach utilizes a novel neural network architecture featuring a Timed Graph Relationformer (TGR) layer combined with a Simplified Structured State Space Sequence (S5) model. The TGR layer effectively processes graph-based observations of the swarm, capturing temporal dependencies and fusing relational information using a learned gating mechanism to generate informative representations for policy learning. Extensive simulations demonstrate that our TGR-based model outperforms baseline graph neural network architectures and exhibits significant zero-shot generalization capabilities across varying swarm sizes and speeds different from those used during training. The trained prober achieves high accuracy in identifying the leader, maintaining performance even in out-of-training distribution scenarios, and showing appropriate confidence levels in its predictions. Real-world experiments with physical robots further validate the approach, confirming successful sim-to-real transfer and robustness to dynamic changes, such as unexpected agent disconnections."})]})}function u(e={}){const{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,r.jsx)(i,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>s,x:()=>o});var a=n(6540);const r={},t=a.createContext(r);function s(e){const i=a.useContext(t);return a.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),a.createElement(t.Provider,{value:i},e.children)}}}]);